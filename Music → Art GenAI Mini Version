"""
Cross-Modal Generative AI (Long Code Skeleton)
================================================

Project: Lyrics → Music and Music → Art
Frameworks: PyTorch (audio), TensorFlow/Keras (image)

This is a full, end-to-end *research-style* codebase in a single file to help you
prototype a complex multimodal system:

- TextEncoder (PyTorch): encodes lyrics into latent embeddings.
- AudioGenerator (PyTorch, WaveNet-like): generates raw audio from latents.
- ImageUNetDiffusion (TensorFlow): diffusion-style image generator conditioned on audio/image/text latents.
- FusionBridge: shared latent space + cross-modal adapters.
- Data pipelines for text–audio and audio–image pairs.
- Training loops (separate and joint), checkpointing, and simple CLAP/contrastive losses.
- Inference demos: lyrics→music (wav), music→art (png), lyrics→art (png).

NOTE: This is a heavy skeleton designed for study. Replace datasets, tweak model sizes,
add real augmentations, and scale up components as needed. It is *not* meant to perfectly
replicate MusicGen or Stable Diffusion; it gives you a faithful, modular starting point.

Usage (example):
-----------------
# 1) Install deps (example; adapt as needed):
#    pip install torch torchaudio torchvision tensorflow tensorflow-io matplotlib soundfile einops sentencepiece transformers
#
# 2) Prepare tiny toy datasets under ./data/
#    data/text_audio/train.csv   -> columns: text_path,audio_path
#    data/audio_image/train.csv  -> columns: audio_path,image_path
#    (Provide your own small .wav/.png for smoke tests.)
#
# 3) Train text→audio (stage A):
#    python cross_modal_generative_ai.py --stage text2audio --train --epochs 2 --batch_size 2
#
# 4) Train audio→image (stage B):
#    python cross_modal_generative_ai.py --stage audio2image --train --epochs 2 --batch_size 2
#
# 5) Joint/fusion finetune (stage C):
#    python cross_modal_generative_ai.py --stage fusion --train --epochs 1 --batch_size 1
#
# 6) Inference:
#    python cross_modal_generative_ai.py --infer lyrics2music --lyrics "dancing in the dark sky" --out_wav out.wav
#    python cross_modal_generative_ai.py --infer music2art --audio data/demo.wav --out_png out.png
#    python cross_modal_generative_ai.py --infer lyrics2art --lyrics "neon city at midnight" --out_png out2.png
#
# Tips:
#  - Start tiny (short audio, 8–16 kHz, 1–2 sec; 64×64 images) to validate pipeline.
#  - Then scale up receptive fields, channels, and dataset sizes.
#
"""

from __future__ import annotations
import os
import math
import time
import json
import random
import argparse
from dataclasses import dataclass
from typing import Optional, Tuple, List

# ----------------------------
# PyTorch (audio + text side)
# ----------------------------
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

# Audio utils
import torchaudio

# Text tokenizer/encoder (you can swap with any HF model)
try:
    from transformers import AutoTokenizer, AutoModel
    _HAS_HF = True
except Exception:
    _HAS_HF = False

# ----------------------------
# TensorFlow / Keras (image side)
# ----------------------------
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# ----------------------------
# Misc
# ----------------------------
import numpy as np
from PIL import Image
import soundfile as sf

# ----------------------------
# Configuration
# ----------------------------
@dataclass
class Config:
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    seed: int = 42

    # Data
    sr: int = 16000               # Audio sample rate
    audio_seconds: float = 2.0    # Short for smoke tests; scale later
    n_mels: int = 64
    image_size: int = 64          # Start small; scale later to 256

    # Batching
    batch_size: int = 2
    num_workers: int = 2

    # Text encoder
    text_model_name: str = "sentence-transformers/all-MiniLM-L6-v2"  # compact; swap as needed
    text_embed_dim: int = 384      # matches model above

    # Audio generator (WaveNet-ish tiny)
    audio_channels: int = 64
    audio_stacks: int = 6
    audio_dilations: List[int] = None  # filled in __post_init__

    # Shared latent / fusion
    shared_latent_dim: int = 256

    # Image UNet (tiny)
    im_base_channels: int = 64
    im_channels_mult: Tuple[int, ...] = (1, 2, 2)
    im_num_res_blocks: int = 2
    im_condition_dim: int = 256

    # Diffusion
    diffusion_timesteps: int = 200
    beta_start: float = 1e-4
    beta_end: float = 2e-2

    # Optimizers
    lr: float = 1e-4

    # Paths
    text_audio_csv: str = "data/text_audio/train.csv"
    audio_image_csv: str = "data/audio_image/train.csv"
    ckpt_dir: str = "checkpoints"

    def __post_init__(self):
        if self.audio_dilations is None:
            # exponentially increasing dilations; repeat stacks
            base = [1, 2, 4, 8, 16, 32]
            self.audio_dilations = base[: self.audio_stacks]
        os.makedirs(self.ckpt_dir, exist_ok=True)


# ----------------------------
# Utilities
# ----------------------------

def set_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def seconds_to_samples(seconds: float, sr: int) -> int:
    return int(round(seconds * sr))


def save_wav(path: str, wav: np.ndarray, sr: int):
    wav = np.clip(wav, -1.0, 1.0)
    sf.write(path, wav, sr)


def save_png(path: str, array: np.ndarray):
    arr = np.clip(array, 0.0, 1.0)
    if arr.ndim == 3 and arr.shape[-1] == 1:
        arr = arr[..., 0]
    img = Image.fromarray((arr * 255).astype(np.uint8))
    img.save(path)


# ----------------------------
# Dataset loaders (toy)
# ----------------------------
class TextAudioDataset(Dataset):
    """Dataset of (text, audio_wav) pairs from a CSV.

    CSV format: text_path,audio_path  per line
    text_path -> a .txt containing lyrics/prompt
    audio_path -> a .wav file
    """

    def __init__(self, csv_path: str, sr: int, seconds: float):
        super().__init__()
        self.sr = sr
        self.samples = seconds_to_samples(seconds, sr)
        self.items = []
        if os.path.exists(csv_path):
            with open(csv_path, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if not line or "," not in line:
                        continue
                    t, a = line.split(",", 1)
                    if os.path.exists(t) and os.path.exists(a):
                        self.items.append((t, a))
        else:
            print(f"[WARN] CSV not found: {csv_path}. Dataset will be empty.")

    def __len__(self):
        return len(self.items)

    def __getitem__(self, idx):
        text_path, audio_path = self.items[idx]
        with open(text_path, "r", encoding="utf-8") as f:
            text = f.read().strip()
        wav, file_sr = torchaudio.load(audio_path)  # [C, T]
        wav = torch.mean(wav, dim=0, keepdim=True)  # mono [1, T]
        if file_sr != self.sr:
            wav = torchaudio.functional.resample(wav, file_sr, self.sr)
        T = wav.shape[-1]
        S = self.samples
        if T < S:
            pad = S - T
            wav = F.pad(wav, (0, pad))
        elif T > S:
            start = random.randint(0, T - S)
            wav = wav[:, start:start+S]
        return text, wav.squeeze(0)  # text (str), wav [T]


class AudioImageDataset(Dataset):
    """Dataset of (audio_wav, image) pairs from CSV.

    CSV format: audio_path,image_path
    """

    def __init__(self, csv_path: str, sr: int, seconds: float, image_size: int):
        super().__init__()
        self.sr = sr
        self.samples = seconds_to_samples(seconds, sr)
        self.image_size = image_size
        self.items = []
        if os.path.exists(csv_path):
            with open(csv_path, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if not line or "," not in line:
                        continue
                    a, i = line.split(",", 1)
                    if os.path.exists(a) and os.path.exists(i):
                        self.items.append((a, i))
        else:
            print(f"[WARN] CSV not found: {csv_path}. Dataset will be empty.")

    def __len__(self):
        return len(self.items)

    def __getitem__(self, idx):
        audio_path, image_path = self.items[idx]
        wav, file_sr = torchaudio.load(audio_path)  # [C, T]
        wav = torch.mean(wav, dim=0, keepdim=True)
        if file_sr != self.sr:
            wav = torchaudio.functional.resample(wav, file_sr, self.sr)
        T = wav.shape[-1]
        S = self.samples
        if T < S:
            pad = S - T
            wav = F.pad(wav, (0, pad))
        elif T > S:
            start = random.randint(0, T - S)
            wav = wav[:, start:start+S]

        img = Image.open(image_path).convert("RGB").resize((self.image_size, self.image_size))
        img = torch.from_numpy(np.array(img).astype(np.float32) / 255.0)  # [H,W,3]
        img = img.permute(2, 0, 1)  # [3,H,W]
        return wav.squeeze(0), img


# ----------------------------
# Text Encoder (PyTorch)
# ----------------------------
class TextEncoder(nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()
        self.cfg = cfg
        if _HAS_HF:
            self.tokenizer = AutoTokenizer.from_pretrained(cfg.text_model_name)
            self.encoder = AutoModel.from_pretrained(cfg.text_model_name)
            hidden = cfg.text_embed_dim
        else:
            # Fallback tiny character-level encoder
            vocab = 128
            hidden = cfg.text_embed_dim
            self.tokenizer = None
            self.encoder = nn.Sequential(
                nn.Embedding(vocab, hidden),
                nn.AdaptiveAvgPool1d(1)
            )
        self.proj = nn.Linear(hidden, cfg.shared_latent_dim)

    def forward(self, texts: List[str]) -> torch.Tensor:
        if _HAS_HF:
            toks = self.tokenizer(texts, padding=True, truncation=True, return_tensors='pt')
            toks = {k: v.to(self.proj.weight.device) for k, v in toks.items()}
            out = self.encoder(**toks)
            # use [CLS] / pooled output if available else mean pool
            if hasattr(out, 'pooler_output') and out.pooler_output is not None:
                pooled = out.pooler_output
            else:
                pooled = out.last_hidden_state.mean(dim=1)
        else:
            # naive ascii encode
            maxlen = max(len(t) for t in texts)
            batch = []
            for t in texts:
                ids = [ord(c) % 128 for c in t]
                ids += [0] * (maxlen - len(ids))
                batch.append(ids)
            x = torch.tensor(batch, dtype=torch.long, device=self.proj.weight.device)  # [B,L]
            x = self.encoder[0](x)  # [B,L,H]
            x = x.transpose(1, 2)   # [B,H,L]
            pooled = self.encoder[1](x).squeeze(-1)  # [B,H]
        return self.proj(pooled)  # [B,shared_latent_dim]


# ----------------------------
# WaveNet-like Audio Generator (PyTorch)
# ----------------------------
class CausalConv1d(nn.Conv1d):
    def __init__(self, in_ch, out_ch, kernel_size, dilation=1):
        padding = (kernel_size - 1) * dilation
        super().__init__(in_ch, out_ch, kernel_size, padding=padding, dilation=dilation)
        self.left_padding = padding

    def forward(self, x):
        out = super().forward(x)
        if self.left_padding:
            return out[:, :, :-self.left_padding]
        return out


class WaveNetBlock(nn.Module):
    def __init__(self, channels: int, dilation: int, cond_dim: int):
        super().__init__()
        self.filter = CausalConv1d(channels, channels, kernel_size=3, dilation=dilation)
        self.gate = CausalConv1d(channels, channels, kernel_size=3, dilation=dilation)
        self.cond_f = nn.Linear(cond_dim, channels)
        self.cond_g = nn.Linear(cond_dim, channels)
        self.res = nn.Conv1d(channels, channels, kernel_size=1)
        self.skip = nn.Conv1d(channels, channels, kernel_size=1)

    def forward(self, x, cond):
        # x: [B,C,T], cond: [B,cond_dim]
        c = cond.unsqueeze(-1)  # [B,cond_dim,1]
        f = torch.tanh(self.filter(x) + self.cond_f(c))
        g = torch.sigmoid(self.gate(x) + self.cond_g(c))
        h = f * g
        res = self.res(h)
        skip = self.skip(h)
        return (x + res), skip


class AudioGenerator(nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()
        C = cfg.audio_channels
        self.input = nn.Conv1d(1, C, kernel_size=1)
        self.blocks = nn.ModuleList([WaveNetBlock(C, d, cfg.shared_latent_dim) for d in cfg.audio_dilations])
        self.post = nn.Sequential(
            nn.ReLU(),
            nn.Conv1d(C, C, kernel_size=1),
            nn.ReLU(),
            nn.Conv1d(C, 1, kernel_size=1),
            nn.Tanh()  # output waveform in [-1,1]
        )

    def forward(self, noise: torch.Tensor, cond: torch.Tensor) -> torch.Tensor:
        # noise: [B,1,T], cond: [B,D]
        x = self.input(noise)
        skips = 0
        for blk in self.blocks:
            x, s = blk(x, cond)
            skips = skips + s
        x = self.post(skips)
        return x  # [B,1,T]


# ----------------------------
# Fusion bridge (PyTorch) — shared latent space + audio/text adapters
# ----------------------------
class FusionBridge(nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()
        D = cfg.shared_latent_dim
        self.text_to_shared = nn.Sequential(nn.Linear(D, D), nn.ReLU(), nn.Linear(D, D))
        self.audio_to_shared = nn.Sequential(nn.Linear(D, D), nn.ReLU(), nn.Linear(D, D))
        self.shared_to_image = nn.Sequential(nn.Linear(D, D), nn.ReLU(), nn.Linear(D, D))

    def text_latent(self, z_text):
        return self.text_to_shared(z_text)

    def audio_latent(self, z_audio):
        return self.audio_to_shared(z_audio)

    def image_cond(self, z_shared):
        return self.shared_to_image(z_shared)


# ----------------------------
# Simple audio encoder to produce latents for contrastive/fusion losses (PyTorch)
# ----------------------------
class TinyAudioEncoder(nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv1d(1, 32, 5, stride=2, padding=2), nn.ReLU(),
            nn.Conv1d(32, 64, 5, stride=2, padding=2), nn.ReLU(),
            nn.Conv1d(64, 128, 5, stride=2, padding=2), nn.ReLU(),
        )
        self.pool = nn.AdaptiveAvgPool1d(1)
        self.head = nn.Linear(128, cfg.shared_latent_dim)

    def forward(self, wav: torch.Tensor):  # [B,T] or [B,1,T]
        if wav.ndim == 2:
            wav = wav.unsqueeze(1)
        x = self.conv(wav)
        x = self.pool(x).squeeze(-1)
        return self.head(x)


# ----------------------------
# Contrastive loss (InfoNCE-style)
# ----------------------------
class InfoNCE(nn.Module):
    def __init__(self, temperature: float = 0.07):
        super().__init__()
        self.temperature = temperature

    def forward(self, a: torch.Tensor, b: torch.Tensor):
        # a,b: [B,D]
        a = F.normalize(a, dim=-1)
        b = F.normalize(b, dim=-1)
        logits = a @ b.t() / self.temperature  # [B,B]
        labels = torch.arange(a.size(0), device=a.device)
        loss_a = F.cross_entropy(logits, labels)
        loss_b = F.cross_entropy(logits.t(), labels)
        return (loss_a + loss_b) * 0.5


# =======================================================
# TensorFlow / Keras — Diffusion-style Image UNet
# =======================================================

def sinusoidal_time_embedding(t, dim):
    half = dim // 2
    freqs = tf.exp(tf.range(half, dtype=tf.float32) * -(math.log(10000.0) / (half - 1)))
    args = tf.cast(tf.expand_dims(t, -1), tf.float32) * freqs
    emb = tf.concat([tf.sin(args), tf.cos(args)], axis=-1)
    if dim % 2 == 1:
        emb = tf.pad(emb, [[0, 0], [0, 1]])
    return emb


def ResBlock(x, channels, cond=None):
    h = layers.Conv2D(channels, 3, padding='same')(x)
    h = layers.Activation('swish')(h)
    if cond is not None:
        # project cond to channels and add
        c = layers.Dense(channels)(cond)
        c = layers.Activation('swish')(c)
        c = layers.Reshape((1, 1, channels))(c)
        h = layers.Add()([h, c])
    h = layers.Conv2D(channels, 3, padding='same')(h)
    h = layers.Activation('swish')(h)
    if x.shape[-1] != channels:
        x = layers.Conv2D(channels, 1)(x)
    return layers.Add()([x, h])


def make_unet(cfg: Config, image_size: int, cond_dim: int):
    inp = keras.Input(shape=(image_size, image_size, 3))
    t_inp = keras.Input(shape=(1,), dtype=tf.int32)
    c_inp = keras.Input(shape=(cond_dim,))

    temb = sinusoidal_time_embedding(tf.cast(t_inp, tf.float32), cond_dim)
    temb = layers.Dense(cond_dim, activation='swish')(temb)

    cond = layers.Concatenate()([temb, c_inp])
    cond = layers.Dense(cond_dim, activation='swish')(cond)

    x = inp
    skips = []
    ch = cfg.im_base_channels
    # Down
    for mult in cfg.im_channels_mult:
        for _ in range(cfg.im_num_res_blocks):
            x = ResBlock(x, ch * mult, cond)
        skips.append(x)
        x = layers.AveragePooling2D()(x)

    # Middle
    x = ResBlock(x, ch * cfg.im_channels_mult[-1], cond)

    # Up
    for mult in reversed(cfg.im_channels_mult):
        x = layers.UpSampling2D()(x)
        x = layers.Concatenate()([x, skips.pop()])
        for _ in range(cfg.im_num_res_blocks):
            x = ResBlock(x, ch * mult, cond)

    out = layers.Conv2D(3, 1)(x)

    model = keras.Model([inp, t_inp, c_inp], out, name="ImageUNet")
    return model


class DiffusionSchedule:
    def __init__(self, cfg: Config, timesteps: int):
        self.timesteps = timesteps
        self.betas = np.linspace(cfg.beta_start, cfg.beta_end, timesteps, dtype=np.float32)
        self.alphas = 1.0 - self.betas
        self.alphas_cumprod = np.cumprod(self.alphas)
        self.sqrt_alphas_cumprod = np.sqrt(self.alphas_cumprod)
        self.sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - self.alphas_cumprod)

# ----------------------------
# Training helpers (PyTorch)
# ----------------------------

def train_text_to_audio(cfg: Config, epochs: int, batch_size: int):
    set_seed(cfg.seed)
    ds = TextAudioDataset(cfg.text_audio_csv, cfg.sr, cfg.audio_seconds)
    dl = DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=cfg.num_workers)

    text_enc = TextEncoder(cfg).to(cfg.device)
    audio_gen = AudioGenerator(cfg).to(cfg.device)
    audio_enc = TinyAudioEncoder(cfg).to(cfg.device)
    fusion = FusionBridge(cfg).to(cfg.device)

    opt = torch.optim.Adam(list(text_enc.parameters()) +
                           list(audio_gen.parameters()) +
                           list(audio_enc.parameters()) +
                           list(fusion.parameters()), lr=cfg.lr)

    contrast = InfoNCE()

    steps = 0
    for ep in range(epochs):
        for batch in dl:
            texts, wav = batch  # texts: list[str], wav: [B,T]
            wav = wav.to(cfg.device)
            B, T = wav.shape

            # 1) text → shared latent
            z_text = text_enc(list(texts))  # [B,D]
            z_shared = fusion.text_latent(z_text)  # [B,D]

            # 2) audio generation (teacher-forced objective: reconstruct wav from noise + cond)
            noise = torch.randn(B, 1, T, device=cfg.device)
            wav_pred = audio_gen(noise, z_shared).squeeze(1)  # [B,T]
            recon_loss = F.l1_loss(wav_pred, wav)

            # 3) contrastive align text and audio latents
            z_audio = audio_enc(wav)  # [B,D]
            loss_cl = contrast(z_shared, z_audio)

            loss = recon_loss + 0.1 * loss_cl

            opt.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(list(audio_gen.parameters()), 1.0)
            opt.step()

            if steps % 10 == 0:
                print(f"[text2audio] ep {ep} step {steps} loss {loss.item():.4f} (recon {recon_loss.item():.4f} cl {loss_cl.item():.4f})")
            steps += 1

        # save lightweight checkpoint
        torch.save({
            'text_enc': text_enc.state_dict(),
            'audio_gen': audio_gen.state_dict(),
            'audio_enc': audio_enc.state_dict(),
            'fusion': fusion.state_dict(),
            'cfg': cfg.__dict__
        }, os.path.join(cfg.ckpt_dir, f"text2audio_ep{ep}.pt"))


# ----------------------------
# Training helpers (TF/Keras)
# ----------------------------

def train_audio_to_image(cfg: Config, epochs: int, batch_size: int):
    set_seed(cfg.seed)
    ds = AudioImageDataset(cfg.audio_image_csv, cfg.sr, cfg.audio_seconds, cfg.image_size)
    # We will iterate dataset in Python and feed TF; for real training, build tf.data.

    schedule = DiffusionSchedule(cfg, cfg.diffusion_timesteps)
    unet = make_unet(cfg, cfg.image_size, cfg.im_condition_dim)
    opt = keras.optimizers.Adam(cfg.lr)

    # Audio encoder to produce conditions (PyTorch — will run on CPU/GPU and feed numpy)
    audio_enc = TinyAudioEncoder(cfg).to(cfg.device).eval()
    for p in audio_enc.parameters():
        p.requires_grad = False

    @tf.function
    def train_step(x0, t, cond, noise):
        with tf.GradientTape() as tape:
            # forward diffusion x_t = sqrt(a_bar)*x0 + sqrt(1-a_bar)*noise
            a_bar = tf.gather(schedule.alphas_cumprod, t)
            a_bar = tf.reshape(a_bar, (-1, 1, 1, 1))
            x_t = tf.sqrt(a_bar) * x0 + tf.sqrt(1.0 - a_bar) * noise
            eps_pred = unet([x_t, tf.reshape(t, (-1, 1)), cond])
            loss = tf.reduce_mean(tf.square(eps_pred - noise))
        grads = tape.gradient(loss, unet.trainable_variables)
        opt.apply_gradients(zip(grads, unet.trainable_variables))
        return loss

    steps = 0
    for ep in range(epochs):
        # simple Python loop dataloader
        idxs = list(range(len(ds)))
        random.shuffle(idxs)
        for i in range(0, len(idxs), batch_size):
            batch_idx = idxs[i:i+batch_size]
            if not batch_idx:
                continue
            wavs = []
            imgs = []
            for j in batch_idx:
                w, im = ds[j]
                wavs.append(w.unsqueeze(0))
                imgs.append(im.unsqueeze(0))
            wavs = torch.cat(wavs, dim=0).to(cfg.device)  # [B, T]
            imgs = torch.cat(imgs, dim=0).cpu().numpy()   # [B,3,H,W]
            imgs = np.transpose(imgs, (0, 2, 3, 1)).astype(np.float32)

            with torch.no_grad():
                z_audio = audio_enc(wavs).cpu().numpy()  # [B,D]
            B = imgs.shape[0]
            t = np.random.randint(0, schedule.timesteps, size=(B,), dtype=np.int32)
            noise = np.random.randn(*imgs.shape).astype(np.float32)

            loss = train_step(tf.convert_to_tensor(imgs),
                              tf.convert_to_tensor(t),
                              tf.convert_to_tensor(z_audio),
                              tf.convert_to_tensor(noise))
            if steps % 10 == 0:
                print(f"[audio2image] ep {ep} step {steps} loss {float(loss):.4f}")
            steps += 1

        unet.save(os.path.join(cfg.ckpt_dir, f"image_unet_ep{ep}.keras"))


# ----------------------------
# Joint fusion finetuning (align text↔audio↔image)
# ----------------------------

def train_fusion(cfg: Config, epochs: int, batch_size: int):
    set_seed(cfg.seed)
    # use text-audio pairs for alignment and simple image regularization if available
    ta_ds = TextAudioDataset(cfg.text_audio_csv, cfg.sr, cfg.audio_seconds)
    ti_dl = DataLoader(ta_ds, batch_size=batch_size, shuffle=True, num_workers=cfg.num_workers)

    text_enc = TextEncoder(cfg).to(cfg.device)
    audio_enc = TinyAudioEncoder(cfg).to(cfg.device)
    fusion = FusionBridge(cfg).to(cfg.device)

    opt = torch.optim.Adam(list(text_enc.parameters()) + list(audio_enc.parameters()) + list(fusion.parameters()), lr=cfg.lr)
    contrast = InfoNCE()

    steps = 0
    for ep in range(epochs):
        for batch in ti_dl:
            texts, wav = batch
            wav = wav.to(cfg.device)
            z_text = text_enc(list(texts))
            z_audio = audio_enc(wav)
            z_t_shared = fusion.text_latent(z_text)
            z_a_shared = fusion.audio_latent(z_audio)

            loss_align = contrast(z_t_shared, z_a_shared)

            opt.zero_grad()
            loss_align.backward()
            opt.step()

            if steps % 10 == 0:
                print(f"[fusion] ep {ep} step {steps} align {loss_align.item():.4f}")
            steps += 1

        torch.save({
            'text_enc': text_enc.state_dict(),
            'audio_enc': audio_enc.state_dict(),
            'fusion': fusion.state_dict(),
            'cfg': cfg.__dict__
        }, os.path.join(cfg.ckpt_dir, f"fusion_ep{ep}.pt"))


# ----------------------------
# Inference: lyrics→music (PyTorch)
# ----------------------------
@torch.no_grad()
def infer_lyrics_to_music(cfg: Config, lyrics: str, out_wav: str, ckpt: Optional[str] = None):
    text_enc = TextEncoder(cfg).to(cfg.device).eval()
    audio_gen = AudioGenerator(cfg).to(cfg.device).eval()
    fusion = FusionBridge(cfg).to(cfg.device).eval()

    # Optionally load ckpt
    if ckpt and os.path.exists(ckpt):
        state = torch.load(ckpt, map_location=cfg.device)
        text_enc.load_state_dict(state['text_enc'])
        audio_gen.load_state_dict(state['audio_gen'])
        fusion.load_state_dict(state['fusion'])

    T = seconds_to_samples(cfg.audio_seconds, cfg.sr)
    z_text = text_enc([lyrics])
    z_shared = fusion.text_latent(z_text)

    noise = torch.randn(1, 1, T, device=cfg.device)
    wav = audio_gen(noise, z_shared).cpu().numpy()[0, 0]
    save_wav(out_wav, wav, cfg.sr)
    print(f"Saved: {out_wav}")


# ----------------------------
# Inference: music→art and lyrics→art (TF diffusion sampling)
# ----------------------------

def p_sample_img(unet, x_t, t, cond, schedule: DiffusionSchedule):
    # Predict epsilon
    t_tensor = tf.convert_to_tensor(np.array([t] * x_t.shape[0], dtype=np.int32))
    eps = unet([x_t, tf.reshape(t_tensor, (-1, 1)), cond])

    beta_t = schedule.betas[t]
    alpha_t = schedule.alphas[t]
    a_bar_t = schedule.alphas_cumprod[t]

    coef1 = 1.0 / math.sqrt(alpha_t)
    coef2 = (1 - alpha_t) / math.sqrt(1 - a_bar_t)
    mean = coef1 * (x_t - coef2 * eps)

    if t > 0:
        noise = tf.random.normal(shape=x_t.shape)
        sigma = math.sqrt(beta_t)
        return mean + sigma * noise
    else:
        return mean


def sample_image(cfg: Config, cond_vec: np.ndarray, out_png: str, ckpt_keras: Optional[str] = None):
    # Load UNet
    if ckpt_keras and os.path.exists(ckpt_keras):
        unet = keras.models.load_model(ckpt_keras, compile=False)
    else:
        unet = make_unet(cfg, cfg.image_size, cfg.im_condition_dim)

    schedule = DiffusionSchedule(cfg, cfg.diffusion_timesteps)
    B = 1
    x = tf.random.normal((B, cfg.image_size, cfg.image_size, 3))
    cond = tf.convert_to_tensor(cond_vec.astype(np.float32))

    for t in reversed(range(schedule.timesteps)):
        x = p_sample_img(unet, x, t, cond, schedule)

    img = x.numpy()[0]
    img = (img - img.min()) / (img.max() - img.min() + 1e-8)
    save_png(out_png, img)
    print(f"Saved: {out_png}")


@torch.no_grad()
def infer_music_to_art(cfg: Config, audio_path: str, out_png: str, keras_ckpt: Optional[str] = None, fusion_pt: Optional[str] = None):
    # Encode audio with TinyAudioEncoder → shared latent → image condition
    audio_enc = TinyAudioEncoder(cfg).to(cfg.device).eval()
    fusion = FusionBridge(cfg).to(cfg.device).eval()
    if fusion_pt and os.path.exists(fusion_pt):
        state = torch.load(fusion_pt, map_location=cfg.device)
        fusion.load_state_dict(state['fusion'])

    wav, sr = torchaudio.load(audio_path)
    wav = torch.mean(wav, dim=0, keepdim=True)
    if sr != cfg.sr:
        wav = torchaudio.functional.resample(wav, sr, cfg.sr)
    T = seconds_to_samples(cfg.audio_seconds, cfg.sr)
    if wav.shape[-1] < T:
        wav = F.pad(wav, (0, T - wav.shape[-1]))
    elif wav.shape[-1] > T:
        wav = wav[:, :T]

    z_audio = audio_enc(wav.to(cfg.device))  # [1,D]
    z_shared = fusion.audio_latent(z_audio)
    z_img = fusion.image_cond(z_shared).cpu().numpy()  # [1,D]

    sample_image(cfg, z_img, out_png, ckpt_keras=keras_ckpt)


@torch.no_grad()
def infer_lyrics_to_art(cfg: Config, lyrics: str, out_png: str, keras_ckpt: Optional[str] = None, fusion_pt: Optional[str] = None):
    text_enc = TextEncoder(cfg).to(cfg.device).eval()
    fusion = FusionBridge(cfg).to(cfg.device).eval()
    if fusion_pt and os.path.exists(fusion_pt):
        state = torch.load(fusion_pt, map_location=cfg.device)
        fusion.load_state_dict(state['fusion'])

    z_text = text_enc([lyrics])
    z_shared = fusion.text_latent(z_text)
    z_img = fusion.image_cond(z_shared).cpu().numpy()

    sample_image(cfg, z_img, out_png, ckpt_keras=keras_ckpt)


# ----------------------------
# CLI
# ----------------------------

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--stage', type=str, default='text2audio', choices=['text2audio','audio2image','fusion'])
    parser.add_argument('--train', action='store_true')
    parser.add_argument('--epochs', type=int, default=1)
    parser.add_argument('--batch_size', type=int, default=2)

    parser.add_argument('--infer', type=str, choices=['lyrics2music','music2art','lyrics2art'])
    parser.add_argument('--lyrics', type=str, default=None)
    parser.add_argument('--audio', type=str, default=None)
    parser.add_argument('--out_wav', type=str, default='out.wav')
    parser.add_argument('--out_png', type=str, default='out.png')

    parser.add_argument('--ckpt_pt', type=str, default=None)
    parser.add_argument('--ckpt_keras', type=str, default=None)

    args = parser.parse_args()
    cfg = Config()

    if args.train:
        if args.stage == 'text2audio':
            train_text_to_audio(cfg, args.epochs, args.batch_size)
        elif args.stage == 'audio2image':
            train_audio_to_image(cfg, args.epochs, args.batch_size)
        elif args.stage == 'fusion':
            train_fusion(cfg, args.epochs, args.batch_size)
        else:
            raise ValueError('Unknown stage')
        return

    if args.infer:
        if args.infer == 'lyrics2music':
            if not args.lyrics:
                raise ValueError('--lyrics is required')
            infer_lyrics_to_music(cfg, args.lyrics, args.out_wav, ckpt=args.ckpt_pt)
        elif args.infer == 'music2art':
            if not args.audio:
                raise ValueError('--audio is required')
            infer_music_to_art(cfg, args.audio, args.out_png, keras_ckpt=args.ckpt_keras, fusion_pt=args.ckpt_pt)
        elif args.infer == 'lyrics2art':
            if not args.lyrics:
                raise ValueError('--lyrics is required')
            infer_lyrics_to_art(cfg, args.lyrics, args.out_png, keras_ckpt=args.ckpt_keras, fusion_pt=args.ckpt_pt)
        return

    # If nothing specified, print help
    parser.print_help()


if __name__ == "__main__":
    main()

