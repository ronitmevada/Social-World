"""
Cross-Modal Generative AI (Long Code Skeleton)
================================================

Project: Lyrics → Music and Music → Art
Frameworks: PyTorch (audio), TensorFlow/Keras (image)

This is a full, end-to-end *research-style* codebase in a single file to help you
prototype a complex multimodal system:

- TextEncoder (PyTorch): encodes lyrics into latent embeddings.
- AudioGenerator (PyTorch, WaveNet-like): generates raw audio from latents.
- ImageUNetDiffusion (TensorFlow): diffusion-style image generator conditioned on audio/image/text latents.
- FusionBridge: shared latent space + cross-modal adapters.
- Data pipelines for text–audio and audio–image pairs.
- Training loops (separate and joint), checkpointing, and simple CLAP/contrastive losses.
- Inference demos: lyrics→music (wav), music→art (png), lyrics→art (png).

NOTE: This is a heavy skeleton designed for study. Replace datasets, tweak model sizes,
add real augmentations, and scale up components as needed. It is *not* meant to perfectly
replicate MusicGen or Stable Diffusion; it gives you a faithful, modular starting point.

Usage (example):
-----------------
# 1) Install deps (example; adapt as needed):
#    pip install torch torchaudio torchvision tensorflow tensorflow-io matplotlib soundfile einops sentencepiece transformers
#
# 2) Prepare tiny toy datasets under ./data/
#    data/text_audio/train.csv   -> columns: text_path,audio_path
#    data/audio_image/train.csv  -> columns: audio_path,image_path
#    (Provide your own small .wav/.png for smoke tests.)
#
# 3) Train text→audio (stage A):
#    python cross_modal_generative_ai.py --stage text2audio --train --epochs 2 --batch_size 2
#
# 4) Train audio→image (stage B):
#    python cross_modal_generative_ai.py --stage audio2image --train --epochs 2 --batch_size 2
#
# 5) Joint/fusion finetune (stage C):
#    python cross_modal_generative_ai.py --stage fusion --train --epochs 1 --batch_size 1
#
# 6) Inference:
#    python cross_modal_generative_ai.py --infer lyrics2music --lyrics "dancing in the dark sky" --out_wav out.wav
#    python cross_modal_generative_ai.py --infer music2art --audio data/demo.wav --out_png out.png
#    python cross_modal_generative_ai.py --infer lyrics2art --lyrics "neon city at midnight" --out_png out2.png
#
# Tips:
#  - Start tiny (short audio, 8–16 kHz, 1–2 sec; 64×64 images) to validate pipeline.
#  - Then scale up receptive fields, channels, and dataset sizes.
#
"""

from __future__ import annotations
import os
import math
import time
import json
import random
import argparse
from dataclasses import dataclass
from typing import Optional, Tuple, List

# ----------------------------
# PyTorch (audio + text side)
# ----------------------------
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

# Audio utils
import torchaudio

# Text tokenizer/encoder (you can swap with any HF model)
try:
    from transformers import AutoTokenizer, AutoModel
    _HAS_HF = True
except Exception:
    _HAS_HF = False

# ----------------------------
# TensorFlow / Keras (image side)
# ----------------------------
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# ----------------------------
# Misc
# ----------------------------
import numpy as np
from PIL import Image
import soundfile as sf

# ----------------------------
# Configuration
# ----------------------------
@dataclass
class Config:
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    seed: int = 42

    # Data
    sr: int = 16000               # Audio sample rate
    audio_seconds: float = 2.0    # Short for smoke tests; scale later
    n_mels: int = 64
    image_size: int = 64          # Start small; scale later to 256

    # Batching
    batch_size: int = 2
    num_workers: int = 2

    # Text encoder
    text_model_name: str = "sentence-transformers/all-MiniLM-L6-v2"  # compact; swap as needed
    text_embed_dim: int = 384      # matches model above

    # Audio generator (WaveNet-ish tiny)
    audio_channels: int = 64
    audio_stacks: int = 6
    audio_dilations: List[int] = None  # filled in __post_init__

    # Shared latent / fusion
    shared_latent_dim: int = 256

    # Image UNet (tiny)
    im_base_channels: int = 64
    im_channels_mult: Tuple[int, ...] = (1, 2, 2)
    im_num_res_blocks: int = 2
    im_condition_dim: int = 256

    # Diffusion
    diffusion_timesteps: int = 200
    beta_start: float = 1e-4
    beta_end: float = 2e-2

    # Optimizers
    lr: float = 1e-4

    # Paths
    text_audio_csv: str = "data/text_audio/train.csv"
    audio_image_csv: str = "data/audio_image/train.csv"
    ckpt_dir: str = "checkpoints"

    def __post_init__(self):
        if self.audio_dilations is None:
            # exponentially increasing dilations; repeat stacks
            base = [1, 2, 4, 8, 16, 32]
            self.audio_dilations = base[: self.audio_stacks]
        os.makedirs(self.ckpt_dir, exist_ok=True)


# ----------------------------
# Utilities
# ----------------------------

def set_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def seconds_to_samples(seconds: float, sr: int) -> int:
    return int(round(seconds * sr))


def save_wav(path: str, wav: np.ndarray, sr: int):
    wav = np.clip(wav, -1.0, 1.0)
    sf.write(path, wav, sr)


def save_png(path: str, array: np.ndarray):
    arr = np.clip(array, 0.0, 1.0)
    if arr.ndim == 3 and arr.shape[-1] == 1:
        arr = arr[..., 0]
    img = Image.fromarray((arr * 255).astype(np.uint8))
    img.save(path)


# ----------------------------
# Dataset loaders (toy)
# ----------------------------
class TextAudioDataset(Dataset):
    """Dataset of (text, audio_wav) pairs from a CSV.

    CSV format: text_path,audio_path  per line
    text_path -> a .txt containing lyrics/prompt
    audio_path -> a .wav file
    """

    def __init__(self, csv_path: str, sr: int, seconds: float):
        super().__init__()
        self.sr = sr
        self.samples = seconds_to_samples(seconds, sr)
        self.items = []
        if os.path.exists(csv_path):
            with open(csv_path, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if not line or "," not in line:
                        continue
                    t, a = line.split(",", 1)
                    if os.path.exists(t) and os.path.exists(a):
                        self.items.append((t, a))
        else:
            print(f"[WARN] CSV not found: {csv_path}. Dataset will be empty.")

    def __len__(self):
        return len(self.items)

    def __getitem__(self, idx):
        text_path, audio_path = self.items[idx]
        with open(text_path, "r", encoding="utf-8") as f:
            text = f.read().strip()
        wav, file_sr = torchaudio.load(audio_path)  # [C, T]
        wav = torch.mean(wav, dim=0, keepdim=True)  # mono [1, T]
        if file_sr != self.sr:
            wav = torchaudio.functional.resample(wav, file_sr, self.sr)
        T = wav.shape[-1]
        S = self.samples
        if T < S:
            pad = S - T
            wav = F.pad(wav, (0, pad))
        elif T > S:
            start = random.randint(0, T - S)
            wav = wav[:, start:start+S]
        return text, wav.squeeze(0)  # text (str), wav [T]


class AudioImageDataset(Dataset):
    """Dataset of (audio_wav, image) pairs from CSV.

    CSV format: audio_path,image_path
    """

    def __init__(self, csv_path: str, sr: int, seconds: float, image_size: int):
        super().__init__()
        self.sr = sr
        self.samples = seconds_to_samples(seconds, sr)
        self.image_size = image_size
        self.items = []
        if os.path.exists(csv_path):
            with open(csv_path, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if not line or "," not in line:
                        continue
                    a, i = line.split(",", 1)
                    if os.path.exists(a) and os.path.exists(i):
                        self.items.append((a, i))
        else:
            print(f"[WARN] CSV not found: {csv_path}. Dataset will be empty.")

    def __len__(self):
        return len(self.items)

    def __getitem__(self, idx):
        audio_path, image_path = self.items[idx]
        wav, file_sr = torchaudio.load(audio_path)  # [C, T]
        wav = torch.mean(wav, dim=0, keepdim=True)
        if file_sr != self.sr:
            wav = torchaudio.functional.resample(wav, file_sr, self.sr)
        T = wav.shape[-1]
        S = self.samples
        if T < S:
            pad = S - T
            wav = F.pad(wav, (0, pad))
        elif T > S:
            start = random.randint(0, T - S)
            wav = wav[:, start:start+S]

        img = Image.open(image_path).convert("RGB").resize((self.image_size, self.image_size))
        img = torch.from_numpy(np.array(img).astype(np.float32) / 255.0)  # [H,W,3]
        img = img.permute(2, 0, 1)  # [3,H,W]
        return wav.squeeze(0), img


# ----------------------------
# Text Encoder (PyTorch)
# ----------------------------
class TextEncoder(nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()
        self.cfg = cfg
        if _HAS_HF:
            self.tokenizer = AutoTokenizer.from_pretrained(cfg.text_model_name)
            self.encoder = AutoModel.from_pretrained(cfg.text_model_name)
            hidden = cfg.text_embed_dim
        else:
            # Fallback tiny character-level encoder
            vocab = 128
            hidden = cfg.text_embed_dim
            self.tokenizer = None
            self.encoder = nn.Sequential(
                nn.Embedding(vocab, hidden),
                nn.AdaptiveAvgPool1d(1)
            )
        self.proj = nn.Linear(hidden, cfg.shared_latent_dim)

    def forward(self, texts: List[str]) -> torch.Tensor:
        if _HAS_HF:
            toks = self.tokenizer(texts, padding=True, truncation=True, return_tensors='pt')
            toks = {k: v.to(self.proj.weight.device) for k, v in toks.items()}
            out = self.encoder(**toks)
            # use [CLS] / pooled output if available else mean pool
            if hasattr(out, 'pooler_output') and out.pooler_output is not None:
                pooled = out.pooler_output
            else:
                pooled = out.last_hidden_state.mean(dim=1)
        else:
            # naive ascii encode
            maxlen = max(len(t) for t in texts)
            batch = []
            for t in texts:
                ids = [ord(c) % 128 for c in t]
                ids += [0] * (maxlen - len(ids))
                batch.append(ids)
            x = torch.tensor(batch, dtype=torch.long, device=self.proj.weight.device)  # [B,L]
            x = self.encoder[0](x)  # [B,L,H]
            x = x.transpose(1, 2)   # [B,H,L]
            pooled = self.encoder[1](x).squeeze(-1)  # [B,H]
        return self.proj(pooled)  # [B,shared_latent_dim]


# ----------------------------
# WaveNet-like Audio Generator (PyTorch)
# ----------------------------
class CausalConv1d(nn.Conv1d):
    def __init__(self, in_ch, out_ch, kernel_size, dilation=1):
        padding = (kernel_size - 1) * dilation
        super().__init__(in_ch, out_ch, kernel_size, padding=padding, dilation=dilation)
        self.left_padding = padding

    def forward(self, x):
        out = super().forward(x)
        if self.left_padding:
            return out[:, :, :-self.left_padding]
        return out


class WaveNetBlock(nn.Module):
    def __init__(self, channels: int, dilation: int, cond_dim: int):
        super().__init__()
        self.filter = CausalConv1d(channels, channels, kernel_size=3, dilation=dilation)
        self.gate = CausalConv1d(channels, channels, kernel_size=3, dilation=dilation)
        self.cond_f = nn.Linear(cond_dim, channels)
        self.cond_g = nn.Linear(cond_dim, channels)
        self.res = nn.Conv1d(channels, channels, kernel_size=1)
        self.skip = nn.Conv1d(channels, channels, kernel_size=1)

    def forward(self, x, cond):
        # x: [B,C,T], cond: [B,cond_dim]
        c = cond.unsqueeze(-1)  # [B,cond_dim,1]
        f = torch.tanh(self.filter(x) + self.cond_f(c))
        g = torch.sigmoid(self.gate(x) + self.cond_g(c))
        h = f * g
        res = self.res(h)
        skip = self.skip(h)
        return (x + res), skip


class AudioGenerator(nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()
        C = cfg.audio_channels
        self.input = nn.Conv1d(1, C, kernel_size=1)
        self.blocks = nn.ModuleList([WaveNetBlock(C, d, cfg.shared_latent_dim) for d in cfg.audio_dilations])
        self.post = nn.Sequential(
            nn.ReLU(),
            nn.Conv1d(C, C, kernel_size=1),
            nn.ReLU(),
            nn.Conv1d(C, 1, kernel_size=1),
            nn.Tanh()  # output waveform in [-1,1]
        )

    def forward(self, noise: torch.Tensor, cond: torch.Tensor) -> torch.Tensor:
        # noise: [B,1,T], cond: [B,D]
        x = self.input(noise)
        skips = 0
        for blk in self.blocks:
            x, s = blk(x, cond)
            skips = skips + s
        x = self.post(skips)
        return x  # [B,1,T]


# ----------------------------
# Fusion bridge (PyTorch) — shared latent space + audio/text adapters
# ----------------------------
class FusionBridge(nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()
        D = cfg.shared_latent_dim
        self.text_to_shared = nn.Sequential(nn.Linear(D, D), nn.ReLU(), nn.Linear(D, D))
        self.audio_to_shared = nn.Sequential(nn.Linear(D, D), nn.ReLU(), nn.Linear(D, D))
        self.shared_to_image = nn.Sequential(nn.Linear(D, D), nn.ReLU(), nn.Linear(D, D))

    def text_latent(self, z_text):
        return self.text_to_shared(z_text)

    def audio_latent(self, z_audio):
        return self.audio_to_shared(z_audio)

    def image_cond(self, z_shared):
        return self.shared_to_image(z_shared)


# ----------------------------
# Simple audio encoder to produce latents for contrastive/fusion losses (PyTorch)
# ----------------------------
class TinyAudioEncoder(nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv1d(1, 32, 5, stride=2, padding=2), nn.ReLU(),
            nn.Conv1d(32, 64, 5, stride=2, padding=2), nn.ReLU(),
            nn.Conv1d(64, 128, 5, stride=2, padding=2), nn.ReLU(),
        )
        self.pool = nn.AdaptiveAvgPool1d(1)
        self.head = nn.Linear(128, cfg.shared_latent_dim)

    def forward(self, wav: torch.Tensor):  # [B,T] or [B,1,T]
        if wav.ndim == 2:
            wav = wav.unsqueeze(1)
        x = self.conv(wav)
        x = self.pool(x).squeeze(-1)
        return self.head(x)


# ----------------------------
# Contrastive loss (InfoNCE-style)
# ----------------------------
class InfoNCE(nn.Module):
    def __init__(self, temperature: float = 0.07):
        super().__init__()
        self.temperature = temperature

    def forward(self, a: torch.Tensor, b: torch.Tensor):
        # a,b: [B,D]
        a = F.normalize(a, dim=-1)
        b = F.normalize(b, dim=-1)
        logits = a @ b.t() / self.temperature  # [B,B]
        labels = torch.arange(a.size(0), device=a.device)
        loss_a = F.cross_entropy(logits, labels)
        loss_b = F.cross_entropy(logits.t(), labels)
        return (loss_a + loss_b) * 0.5


# =======================================================
# TensorFlow / Keras — Diffusion-style Image UNet
# =======================================================

def sinusoidal_time_embedding(t, dim):
    half = dim // 2
    freqs = tf.exp(tf.range(half, dtype=tf.float32) * -(math.log(10000.0) / (half - 1)))
    args = tf.cast(tf.expand_dims(t, -1), tf.float32) * freqs
    emb = tf.concat([tf.sin(args), tf.cos(args)], axis=-1)
    if dim % 2 == 1:
        emb = tf.pad(emb, [[0, 0], [0, 1]])
    return emb


def ResBlock(x, channels, cond=None):
    h = layers.Conv2D(channels, 3, padding='same')(x)
    h = layers.Activation('swish')(h)
    if cond is not None:
        # project cond to channels and add
        c = layers.Dense(channels)(cond)
        c = layers.Activation('swish')(c)
        c = layers.Reshape((1, 1, channels))(c)
        h = layers.Add()([h, c])
    h = layers.Conv2D(channels, 3, padding='same')(h)
    h = layers.Activation('swish')(h)
    if x.shape[-1] != channels:
        x = layers.Conv2D(channels, 1)(x)
    return layers.Add()([x, h])


def make_unet(cfg: Config, image_size: int, cond_dim: int):
    inp = keras.Input(shape=(image_size, image_size, 3))
    t_inp = keras.Input(shape=(1,), dtype=tf.int32)
    c_inp = keras.Input(shape=(cond_dim,))

    temb = sinusoidal_time_embedding(tf.cast(t_inp, tf.float32), cond_dim)
    temb = layers.Dense(cond_dim, activation='swish')(temb)

    cond = layers.Concatenate()([temb, c_inp])
    cond = layers.Dense(cond_dim, activation='swish')(cond)

    x = inp
    skips = []
    ch = cfg.im_base_channels
    # Down
    for mult in cfg.im_channels_mult:
        for _ in range(cfg.im_num_res_blocks):
            x = ResBlock(x, ch * mult, cond)
        skips.append(x)
        x = layers.AveragePooling2D()(x)

    # Middle
    x = ResBlock(x, ch * cfg.im_channels_mult[-1], cond)

    # Up
    for mult in reversed(cfg.im_channels_mult):
        x = layers.UpSampling2D()(x)
        x = layers.Concatenate()([x, skips.pop()])
        for _ in range(cfg.im_num_res_blocks):
            x = ResBlock(x, ch * mult, cond)

    out = layers.Conv2D(3, 1)(x)

    model = keras.Model([inp, t_inp, c_inp], out, name="ImageUNet")
    return model


class DiffusionSchedule:
    def __init__(self, cfg: Config, timesteps: int):
        self.timesteps = timesteps
        self.betas = np.linspace(cfg.beta_start, cfg.beta_end, timesteps, dtype=np.float32)
        self.alphas = 1.0 - self.betas
        self.alphas_cumprod = np.cumprod(self.alphas)
        self.sqrt_alphas_cumprod = np.sqrt(self.alphas_cumprod)
        self.sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - self.alphas_cumprod)


